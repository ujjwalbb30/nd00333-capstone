{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning using HyperDrive\n",
    "\n",
    "TODO: Import Dependencies. In the cell below, import all the dependencies that you will need to complete the project.\n",
    "\n",
    "With the help of python notebooks provided in 1st and 2nd project of this nanodegree program ('Optimizing a pipeline in Azure' and 'Operationalizing machine learning'), I have imported following basic dependencies required to complete this project. Any other specific dependecy will be imported as we proceed further in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1607766840670
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK Version: 1.19.0\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from matplotlib import pyplot as pyplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import pkg_resources\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "from azureml.pipeline.steps import AutoMLStep\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK Version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "TODO: Get data. In the cell below, write code to access the data you will be using in this project. Remember that the dataset needs to be external.\n",
    "\n",
    "I will be using the 'Heart Failure Clinical Data' which consists of 12 features ( age, anaemia, creatinine_phosphokinase, diabetes, ejection_fraction, high_blood_pressure, platelets, serum_creatinine, serum_sodium, sex, smoking, time ) which can be used to predict mortality by heart failure. There are total of 299 input rows in the dataset with 0 null entries.\n",
    "\n",
    "The 12 features are as follows:\n",
    "\n",
    "(1) age\n",
    "\n",
    "(2) anaemia i.e. decrease of red blood cells or hemoglobin (boolean)\n",
    "\n",
    "(3) creatining_phosphokinase i.e. level of the CPK enzyme in the blood (mcg/L)\n",
    "\n",
    "(4) diabetes i.e. if the patient has diabetes or not (boolean)\n",
    "\n",
    "(5) ejection_fraction i.e. percentage of blood leaving the heart at each contraction (percentage)\n",
    "\n",
    "(6) high_blood_pressure i.e. if the patient has hypertension (boolean)\n",
    "\n",
    "(7) platelets i.e. platelets in the blood (kiloplatelets/mL)\n",
    "\n",
    "(8) serum_creatinine i.e. level of serum creatinine in the blood (mg/dL)\n",
    "\n",
    "(9) serum_sodium i.e. level of serum sodium in the blood (mEq/L)\n",
    "\n",
    "(10) sex i.e. woman or man (binary)\n",
    "\n",
    "(11) smoking i.e. if the patient smokes or not (boolean)\n",
    "\n",
    "(12) time i.e. follow-up period (days)\n",
    "\n",
    "We will be predicting the following output:\n",
    "\n",
    "DEATH_EVENT i.e if the patient deceased during the follow-up period (boolean)\n",
    "\n",
    "A machine learning classification model on this dataset will be helpful for early detection of people with cardiovascular disease or those who are at high risk of cardiovascular disease.\n",
    "\n",
    "SOURCE : https://www.kaggle.com/andrewmvd/heart-failure-clinical-data\n",
    "\n",
    "TODO: Get data. In the cell below, write code to access the data you will be using in this project. Remember that the dataset needs to be external.\n",
    "\n",
    "I have already registered the dataset after downloading it from kaggle. So, I will be using the name and description that I saved the dataset with, to import it in my experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1607766867103
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# creating a hyperdrive experiment in our workspace\n",
    "\n",
    "# initializing a workspace\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')\n",
    "\n",
    "# choosing a name for experiment\n",
    "experiment_name = 'capstone-hyperdrive-experiment'\n",
    "project_folder = './pipeline-project'\n",
    "\n",
    "# creating the experiment\n",
    "experiment=Experiment(ws, experiment_name)\n",
    "experiment.start_logging()\n",
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an AMLCompute cluster for running the experiment\n",
    "\n",
    "# importing required dependencies\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choosing a name for our CPU cluster\n",
    "amlcompute_cluster_name = \"aml-hyper\"\n",
    "\n",
    "# Verifying that cluster does not exist already\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=amlcompute_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2', max_nodes=4)\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, compute_config)\n",
    "\n",
    "compute_target.wait_for_completion(show_output=True, min_node_count = 1, timeout_in_minutes = 10)\n",
    "compute_target.get_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering the dataset's name and description in 'key' and 'description_text' respectively\n",
    "\n",
    "key = 'heart-failure-clinical-data'\n",
    "description_text = 'heart failure predictions'\n",
    "\n",
    "# importing the dataset for use\n",
    "dataset = ws.datasets[key]\n",
    "\n",
    "# converting the imported dataset to pandas dataframe for analyzing purpose\n",
    "df = dataset.to_pandas_dataframe()\n",
    "\n",
    "# analyzing the dataframe\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598531923519
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Hyperdrive Configuration\n",
    "\n",
    "TODO: Explain the model you are using and the reason for chosing the different hyperparameters, termination policy and config settings.\n",
    "\n",
    "The model I chose for the classification purpose of the heart failure clinical dataset is explained below. I have tried to explain the pipeline architecture, hyperparameter tuning, and classification algorithm:\n",
    "\n",
    "(1) The pipeline is created using HyperDriveConfig which requires an estimator, an early termination policy, a parameter sampler, a primary metric name, a primary metric goal and a value for maximum total runs.\n",
    "\n",
    "(2) A parameter sampler is created using RandomParameterSampling which generates a set values (equal to value of maximum total runs) of C and max_iter to be used in child runs of experiment. For C, a continuous set of values ranging from 0.0005 to 1.0 is used and for max_iter, a discrete set of values is used which includes 50,100,150,200 and 250.\n",
    "\n",
    "(3) BanditPolicy is used as early termination policy with evalution_interval as 5, slack_amount as 0.2 and delay_evalution as 5. This means if Run X is the currently best performing run with an accuracy of 0.9 after 5 intervals, then any run with an accuracy less than 0.7 (0.9 - 0.2) after 5 iterations will be terminated, and the delay_evaluation will delay the first termination policy evaluation for 5 sequences.\n",
    "\n",
    "(4) An SKLearn estimator is used with train.py as training script (in which the features and labels are first segregated and the dataset is split into train and test using train_test_split module) for Scikit-Learn experiments which trains a Logistic Regression model on heart failure clinical data with varying sets of values of C and max_iter supplied by the parameter sampler (RandomParameterSampling in our case).\n",
    "\n",
    "The benefits of the parameter sampler I chose are mentioned below:\n",
    "\n",
    "The parameter sampler chosen is RandomParameterSampling which selects hyperparameter values randomly from the defined search space. RandomParameterSampling results in good results without consuming too much time.\n",
    "\n",
    "The benefits of the early stopping policy I chose are given below:\n",
    "\n",
    "The early stopping policy chosen is BanditPolicy with evalution_interval as 5, slack_amount as 0.2 and delay_evalution as 5. This means if Run X is the currently best performing run with an accuracy of 0.9 after 5 intervals, then any run with an accuracy less than 0.7 (0.9 - 0.2) after 5 iterations will be terminated, and the delay_evaluation will delay the first termination policy evaluation for 5 sequences. This means I will not lose promising jobs and also the jobs with poor performance will be terminated early hence saving computation time and costs.\n",
    "\n",
    "The parameters of HyperDriveConfig are explained as below:\n",
    "\n",
    "(1) estimator: It is the model estimator to be used to run the model. I have defined an SKLearn estimator below as 'est' and I will use it as 'estimator' parameter.\n",
    "\n",
    "(2) hyperparameter_sampling: It is the sampler that will create the instance of hyperparameters to be used for each sample run. I have defined a RandomParameterSampling below as 'param_sampling' and I will use it as 'hyperparameter_sampling' parameter.\n",
    "\n",
    "(3) policy: It is the early termination policy that will be used to terminate the experiment if no improvement in primary metric is witnessed after some runs. I have defined a BanditPolicy below as 'et_policy' and I will use it as 'policy' parameter.\n",
    "\n",
    "(4) primary_metric_name: it is the name of the metric on the basis of which performance of different models will be judged. I will be using 'AUC_weighted' as the 'primary_metric_name' parameter. AUC means the area under the Receiver Operating Characteristic Curve which plots the relationship between true positive rate and false positive rate. Since our dataset doesn't have high class imbalance, we can use ROC method for judging the performance of a model. I will use AUC_weighted in order to mitigate the effects of whatever little imbalance is there in the dataset. AUC_weighted is the arithmetic mean of the score for each class, weighted by the number of true instances in each class.\n",
    "\n",
    "(5) primary_metric_goal: In order to get the best model for our classification task, my goal is to maximize the 'AUC_weighted' metric hence I will enter 'PrimaryMetricGoal.MAXIMIZE'as 'primary_metric_goal' parameter.\n",
    "\n",
    "(6) max_total_runs: It is the maximum number of child runs that will be executed in the experiment to find the best model for the task intended. I will enter '25' as the 'max_total_runs' parameter which will produce a good and acceptable result in less amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1607766987227
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# importing required dependencies\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive.policy import BanditPolicy\n",
    "from azureml.train.hyperdrive.sampling import RandomParameterSampling\n",
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
    "from azureml.train.hyperdrive.parameter_expressions import uniform, choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598544893076
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Creating an early termination policy\n",
    "et_policy = BanditPolicy(evaluation_interval=5, slack_factor=None, slack_amount=0.2, delay_evaluation=5)\n",
    "\n",
    "# Creating the different parameters that will be used during training\n",
    "param_sampling = RandomParameterSampling({\"C\": uniform(0.0005, 1.0),\"max_iter\": choice(50, 100, 150, 200, 250)})\n",
    "\n",
    "# Create the environment\n",
    "#sklearn_env = Environment.get(workspace=ws, name=\"AzureML-Tutorial\")\n",
    "#src = ScriptRunConfig(source_directory='.', script='train.py', compute_target = compute_target, environment=sklearn_env)\n",
    "#hyperdrive_run_config = HyperDriveConfig( run_config=src, hyperparameter_sampling=param_sampling, policy=early_termination_policy, primary_metric_name = \"Accuracy\", primary_metric_goal = PrimaryMetricGoal.MAXIMIZE, max_total_runs = 100, max_concurrent_runs = 2)\n",
    "# Creating an estimator and hyperdrive config\n",
    "est = SKLearn(source_directory = 'training', entry_script = 'train.py', compute_target = compute_cluster)\n",
    "\n",
    "hyperdrive_run_config = HyperDriveConfig(estimator=est,hyperparameter_sampling=param_sampling,policy=et_policy,primary_metric_name='AUC_weighted',primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,max_total_runs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598544897941
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Submit your experiment\n",
    "run = experiment.submit(config = hyperdrive_run_config, show_output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598544898497
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Run Details\n",
    "\n",
    "OPTIONAL: Write about the different models trained and their performance. Why do you think some models did better than others?\n",
    "\n",
    "TODO: In the cell below, use the `RunDetails` widget to show the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598546648408
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# importing required dependencies\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waiting for completion of run while showing its output\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model\n",
    "\n",
    "TODO: In the cell below, get the best model from the hyperdrive experiments and display all the properties of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598546650307
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve best model from Hyperdrive Run\n",
    "\n",
    "# importing required dependencies\n",
    "import joblib\n",
    "\n",
    "best_run = run.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "parameter_values = best_run.get_details()['runDefinition']['arguments']\n",
    "best_run_model = best_run.get_details()['runDefinition']\n",
    "print('Best Run Id:',best_run.id)\n",
    "print('\\n Accuracy:', best_run_metrics['AUC_weighted'])\n",
    "print('\\n parameter values:',parameter_values)\n",
    "print('\\n details:',best_run_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598546657829
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Save the best model\n",
    "joblib.dump(best_run_model,'best_hyperdrive_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Model Deployment\n",
    "\n",
    "Remember you have to deploy only one of the two models you trained.. Perform the steps in the rest of this notebook only if you wish to deploy this model.\n",
    "\n",
    "TODO: In the cell below, register the model, create an inference config and deploy the model as a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# registering the best automl model\n",
    "\n",
    "description = 'heart failure predictions'\n",
    "tags = None\n",
    "\n",
    "model = remote_run.register_model(description = description, tags = tags)\n",
    "\n",
    "print(remote_run.model_id)\n",
    "\n",
    "# importing required dependencies\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "# loading a curated environment from workspace\n",
    "\n",
    "env = Environment.get(ws, \"AzureML-AutoML\")\n",
    "\n",
    "# specifying scikit-learn as dependency\n",
    "for pip_package in [\"scikit-learn\"]:\n",
    "    env.python.conda_dependencies.add_pip_package(pip_package)\n",
    "\n",
    "# creating an inference config\n",
    "inference_config = InferenceConfig(entry_script='entry_script.py', environment=env)\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1, enable_app_insights = True)\n",
    "\n",
    "# naming the service to be deployed\n",
    "aci_service_name = 'automl-heart-failure-predictions'\n",
    "print(aci_service_name)\n",
    "\n",
    "# deploying the model\n",
    "aci_service = Model.deploy(ws, aci_service_name, [model], inference_config, aciconfig)\n",
    "aci_service.wait_for_deployment(show_output = True)\n",
    "print(aci_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: In the cell below, send a request to the web service you deployed to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required dependencies\n",
    "from numpy import array\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# creating a test sample\n",
    "data = {\"data\": [{\"age\":60.000000,\"anaemia\":0.000000,\"creatinine_phosphokinase\":250.000000,\"diabetes\":0.000000,\"ejection_fraction\":38.000000,\"high_blood_pressure\":0.000000,\"platelets\":262000.000000,\"serum_creatinine\":1.10000,\"serum_sodium\":137.000000,\"sex\":1.000000,\"smoking\":0.00000,\"time\":115.000000}]}\n",
    "td = json.dumps(data)\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "# sending request to test the deployed webservice\n",
    "resp = requests.post(aci_service.scoring_uri, td, headers=headers)\n",
    "print(resp.json())\n",
    "y_pred = (json.loads(resp.text))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: In the cell below, print the logs of the web service and delete the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the logs of deployed web service\n",
    "dep_logs = aci_service.get_logs()\n",
    "for l in dep_logs.split('\\n'):\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting a web service\n",
    "aci_service.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete compute cluster\n",
    "compute_target.delete()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
